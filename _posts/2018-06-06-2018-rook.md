---
title: Système de Stockage Rook
date: 2018-06-06 00:00:00 Z
layout: post
description: ROOK
author: Mehdi EL KOUHEN
toc: false
---

[Rook](https://rook.io/) est un orchestrateur de système de stockage distribués pour Kubernetes. En tant qu'orchestrateur, Rook automatise les processus de gestion des espaces de stockage gérés : provisionning, configuration, déploiement, scaling, mise à jour, gestion des pannes, ... 

Pour le moment, Rook permet une intégration du système de stockage distribué [Ceph](https://ceph.com/).

Via Ceph, Rook fournit trois types de stockage : 

* Block : espace de stockage dédié à un POD
* Object : espace de stockage partagée accessible via une API compatible S3
* Système de Fichiers : espace de stockage Fichier partagée entre PODs

Dans ce Post, nous décrivons la première intégration de Rook dans notre Usine Logicielle. Nous y intégrons un espace de stockage "Système de Fichiers" (notamment pour remplacer les espaces de stockage NFS qui ne sont pas redondés dans notre usine).

## Architecture 

[L'Architecture de Rook](https://github.com/rook/rook/tree/master/Documentation) est basée sur les concepts suivants :

* Opérateur Rook : automatise la configuration des espaces de stockage et monitore le cluster
* Agents Rook : configure l'intégration des volumes Rook dans le contrôleur de volume Kubernetes
* Cluster Rook : gère les espaces de stockage Ceph

![Architecture Rook]({{ "/assets/images/rook-architecture.png" | absolute_url}})

Note : Un [Opérateur](https://coreos.com/operators/) est un mécanisme de de packaging, déploiement, gestion d'une application Kubernetes.

## Déploiement

Le déploiement de Rook est réalisé par étapes : 

* Déployer l'Opérateur Rook (par exemple via un chart helm [rook-ceph](https://rook.io/docs/rook/master/helm-operator.html))
* Déployer le [cluster Rook](https://rook.io/docs/rook/master/quickstart.html)

## Création d'un Système de Fichier

Pour installer un système de fichier Ceph, il faut déployer un [CRD Ceph](https://rook.io/docs/rook/master/ceph-filesystem-crd.html).

La définition du CRD contient notamment 

* Son type (FileSystem)
* Son nom (myfs).

```yaml
apiVersion: ceph.rook.io/v1alpha1
kind: Filesystem
metadata:
  name: myfs
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 3
  dataPools:
    - erasureCoded:
       dataChunks: 2
       codingChunks: 1
  metadataServer:
    activeCount: 1
    activeStandby: true
```

Note : Un [CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) est un mécanisme de Kubernetes qui permet de définir de nouveaux types de Ressources. Dans le cas présent, le type de ressources Filesystem n'est pas définie nativement par Kubernetes.

## Exemple d'utilisation 

Le deployment ci-dessous définit le déploiement d'une base de données postgres. Il est extrait du projet [books-postgres-run](https://github.com/SofteamOuest/books-postgres-run). 

Dans cet exemple, le répertoire /var/lib/postgresql/data contenant les données de la base est monté sur le volume nommé postgres-fs : 

* Type de système de fichiers : ceph
* Nom du système de fichiers : myfs
* Chemin dans le système de fichier monté : /postgresql

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: books-postgres
spec:
  template:
    metadata:
     labels:
        app: books-postgres
    spec:
      containers:
      - name: books-postgres
        image: registry.k8.wildwidewest.xyz/repository/docker-repository/pocs/books-postgres:${IMAGE}
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-fs
          mountPath: /var/lib/postgresql/data
      imagePullSecrets:
      - name: regsecret
      volumes:
      - name: postgres-fs
        flexVolume:
          driver: ceph.rook.io/rook
          fsType: ceph # type du système de fichiers
          options:
            fsName: myfs # name of the filesystem specified in the filesystem CRD.
            clusterNamespace: rook-ceph
            path: /postgresql # chemin dans le montage
```

Voici l'équivalent de ce deployment basé sur un PVC (Persistent Volume Claim Kubernetes). Le PVC doit être associé (bindé) à un PV (Persistent Volume). Un PV correspond à un espace de stockage réél (exemple : montage NFS) 

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: books-postgres
spec:
  template:
    metadata:
     labels:
        app: books-postgres
    spec:
      containers:
      - name: books-postgres
        image: registry.k8.wildwidewest.xyz/repository/docker-repository/pocs/books-postgres:${IMAGE}
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: books-postgres-home
          mountPath: /var/lib/postgresql/data
      imagePullSecrets:
      - name: regsecret
      volumes:
      - name: books-postgres-home
        persistentVolumeClaim:
          claimName: books-postgres-pvc
```

## Test du montage

Rook fournit une [toolbox](https://rook.io/docs/rook/master/toolbox.html) (un POD) qui donne accès aux outils Ceph.

La connexion au POD toolbox donne accès aux outils Ceph.

```yaml
kubectl -n rook-ceph exec -it rook-ceph-tools bash
```

Pour déterminer l'état du cluster Ceph, il suffit d'exécuter

```bash
ceph status
```

```yaml
cluster:
  id:     4f98a247-b2ab-467d-ba7d-d057c903a9b4
  health: HEALTH_WARN
 
services:
  mon: 3 daemons, quorum rook-ceph-mon0,rook-ceph-mon1,rook-ceph-mon2
  mgr: rook-ceph-mgr0(active)
  mds: myfs-1/1/1 up  {0=mwtwcw=up:active}, 1 up:standby-replay
  osd: 6 osds: 5 up, 5 in
 
data:
  pools:   2 pools, 200 pgs
  objects: 1343 objects, 215 MB
  usage:   5883 MB used, 74136 MB / 80019 MB avail
  pgs:     200 active+clean
 
io:
  client:   1619 B/s rd, 2 op/s rd, 0 op/s wr
```

Le montage du filesystem créé (sur /tmp/postgres par exemple) se fait par appel de mount :

```bash
mon_endpoints=$(grep mon_host /etc/ceph/ceph.conf | awk '{print $3}')
my_secret=$(grep key /etc/ceph/keyring | awk '{print $3}')
mount -t ceph -o name=admin,secret=$my_secret $mon_endpoints:/ /tmp/postgres/
```
